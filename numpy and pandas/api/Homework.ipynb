{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1.\n",
    "#### Обязательная часть\n",
    "Будем парсить страницу со свежеми новостям на habr.com/ru/all/.\n",
    "\n",
    "Вам необходимо собирать только те статьи, в которых встречается хотя бы одно требуемое ключевое слово. Эти слова определяем в начале кода в переменной, например:\n",
    "\n",
    "KEYWORDS = ['python', 'парсинг']\n",
    "\n",
    "Поиск вести по всей доступной preview-информации (это информация, доступная непосредственно с текущей страницы).\n",
    "\n",
    "В итоге должен формироваться датафрейм вида: <дата> - <заголовок> - <ссылка>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date                                              title  \\\n",
      "0  2021-07-05  Спектрометр своими руками за 5 долларов и немн...   \n",
      "0  2021-07-05  Нейродайджест: главное из области машинного об...   \n",
      "0  2021-07-05  Учимся читать код, изучая стандартную библиоте...   \n",
      "\n",
      "                                                link  \n",
      "0  https://habr.com/ru/company/skillfactory/blog/...  \n",
      "0                   https://habr.com/ru/post/566236/  \n",
      "0    https://habr.com/ru/company/vdsina/blog/566134/  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def add_data(date_, name_, datas):\n",
    "    dict_datas = {\"января\":\"01\",\"февраля\":\"02\",\"марта\":\"03\",\"апреля\":\"04\",\"мая\":\"05\",\"июня\":\"06\",\n",
    "                  \"июля\":\"07\",\"августа\":\"08\",\"сентября\":\"09\",\"октября\":\"10\",\"ноября\":\"11\",\"декабря\":\"12\"}\n",
    "    if \"сегодня\" in date_:\n",
    "        date = datetime.now().date()\n",
    "    elif \"вчера\" in date_:\n",
    "        date_ = datetime.now().date() - pd.offsets.Day(1)\n",
    "    else:\n",
    "        month = dict_datas[(re.findall(r'\\d{1,2}\\s(.+)\\s\\d{4}.+',date_)[0])]\n",
    "        date = datetime.strptime(re.sub(r'(\\d{1,2})\\s(.*)\\s(\\d{4})(.*)', r'\\3-'+month+r'-\\1',date_), \"%Y-%m-%d\")\n",
    "        \n",
    "    row = {'date': date, 'title': name_.text, 'link': name_.get('href')}\n",
    "    datas = pd.concat([datas, pd.DataFrame([row])]) \n",
    "    return datas\n",
    "\n",
    "def pars_habr(KEYWORDS):  \n",
    "    datas = pd.DataFrame()\n",
    "    res = requests.get('https://habr.com/ru/all/')\n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "    posts = soup.find_all('article', class_='post')\n",
    "    for post in posts:\n",
    "        post_time = post.find('span', class_='post__time')\n",
    "        post_link = post.find('a', class_='post__title_link')\n",
    "        post_text = ((post.find('div', class_='post__text')).text).lower()\n",
    "        post_hubs = post.find_all('a', class_='hub-link')\n",
    "        \n",
    "        if any([KEYWORD in post_text for KEYWORD in KEYWORDS]) or any([KEYWORD in ((post_link.text).lower()) for KEYWORD in KEYWORDS]):\n",
    "            datas = add_data(post_time.text,post_link,datas)\n",
    "        else:\n",
    "            for hub in post_hubs:\n",
    "                if any([KEYWORD in ((hub.text).lower()) for KEYWORD in KEYWORDS]):\n",
    "                    datas = add_data(post_time.text, post_link, datas)\n",
    "    return(datas)\n",
    "\n",
    "def main():\n",
    "    KEYWORDS = ['python', 'парсинг','agile', 'тестирование','здоровье']\n",
    "    data_frame = pars_habr(KEYWORDS)\n",
    "    print(data_frame)\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Дополнительная часть (необязательная)\n",
    "Улучшить скрипт так, чтобы он анализировал не только preview-информацию статьи, но и весь текст статьи целиком.\n",
    "\n",
    "Для этого потребуется получать страницы статей и искать по тексту внутри этой страницы.\n",
    "\n",
    "Итоговый датафрейм формировать со столбцами: <дата> - <заголовок> - <ссылка> - <текст_статьи>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date                                              title  \\\n",
      "0  2021-07-05  Спектрометр своими руками за 5 долларов и немн...   \n",
      "0  2021-07-05  Нейродайджест: главное из области машинного об...   \n",
      "0  2021-07-05  Учимся читать код, изучая стандартную библиоте...   \n",
      "0  2021-07-05              Издательство Питер. Колонка редактора   \n",
      "0  2021-07-05  Уроки, которые мы вынесли из опыта управления ...   \n",
      "\n",
      "                                                link  \\\n",
      "0  https://habr.com/ru/company/skillfactory/blog/...   \n",
      "0                   https://habr.com/ru/post/566236/   \n",
      "0    https://habr.com/ru/company/vdsina/blog/566134/   \n",
      "0     https://habr.com/ru/company/piter/blog/566196/   \n",
      "0   https://habr.com/ru/company/timeweb/blog/566212/   \n",
      "\n",
      "                                                text  \n",
      "0  \\nв освоении физики лабораторные эксперименты ...  \n",
      "0  \\nначнем подборку с новостей из области nlp. я...  \n",
      "0  \\n\\n\\r\\nитак, вы уже продвинутый новичок — вы ...  \n",
      "0  \\n\\r\\nпривет, хаброжители! предлагаем ознакоми...  \n",
      "0  \\nмы в dropbox считаем, что управление инциден...  \n"
     ]
    }
   ],
   "source": [
    "def add_data_with_text(date_, name_, datas, text):\n",
    "    dict_datas = {\"января\":\"01\",\"февраля\":\"02\",\"марта\":\"03\",\"апреля\":\"04\",\"мая\":\"05\",\"июня\":\"06\",\n",
    "                  \"июля\":\"07\",\"августа\":\"08\",\"сентября\":\"09\",\"октября\":\"10\",\"ноября\":\"11\",\"декабря\":\"12\"}\n",
    "    if \"сегодня\" in date_:\n",
    "        date = datetime.now().date()\n",
    "    elif \"вчера\" in date_:\n",
    "        date_ = datetime.now().date() - pd.offsets.Day(1)\n",
    "    else:\n",
    "        month = dict_datas[(re.findall(r'\\d{1,2}\\s(.+)\\s\\d{4}.+',date_)[0])]\n",
    "        date = datetime.strptime(re.sub(r'(\\d{1,2})\\s(.*)\\s(\\d{4})(.*)', r'\\3-'+month+r'-\\1',date_), \"%Y-%m-%d\")\n",
    "    row = {'date': date, 'title': name_.text, 'link': name_.get('href'), 'text': text}\n",
    "    datas = pd.concat([datas, pd.DataFrame([row])]) \n",
    "    return datas\n",
    "\n",
    "\n",
    "def get_post_text( link):\n",
    "    soup = BeautifulSoup((requests.get(link)).text, 'html.parser')\n",
    "    post = soup.find('div', class_='post__body')\n",
    "    return(post.text.lower())\n",
    "\n",
    "\n",
    "def pars_habr_posts(KEYWORDS):  \n",
    "    datas = pd.DataFrame()\n",
    "    res = requests.get('https://habr.com/ru/all/')\n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "    posts = soup.find_all('article', class_='post')\n",
    "    for post in posts:\n",
    "        post_time = post.find('span', class_='post__time')\n",
    "        post_link = post.find('a', class_='post__title_link')\n",
    "        post_text = get_post_text( post_link.get('href'))\n",
    "        post_hubs = post.find_all('a', class_='hub-link')\n",
    "        if any([KEYWORD in post_text for KEYWORD in KEYWORDS]) or any([KEYWORD in ((post_link.text).lower()) for KEYWORD in KEYWORDS]) :\n",
    "            datas = add_data_with_text(post_time.text,post_link,datas,post_text) \n",
    "        else:\n",
    "            for hub in post_hubs:\n",
    "                if any([KEYWORD in ((hub.text).lower()) for KEYWORD in KEYWORDS]):\n",
    "                    datas = add_data_with_text(post_time.text, post_link, datas,post_text)\n",
    "    return(datas)\n",
    "\n",
    "def main():\n",
    "    KEYWORDS = ['python', 'парсинг','agile', 'тестирование','здоровье']\n",
    "    data_frame = pars_habr_posts(KEYWORDS)\n",
    "    print(data_frame)\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2.\n",
    "#### Обязательная часть\n",
    "Написать скрипт, который будет проверять список e-mail адресов на утечку при помощи сервиса Avast Hack Ckeck. Список email-ов задаем переменной в начале кода:\n",
    "EMAIL = [xxx@x.ru, yyy@y.com]\n",
    "\n",
    "В итоге должен формироваться датафрейм со столбцами: <дата утечки> - <источник утечки> - <описание утечки>\n",
    "\n",
    "Подсказка: сервис работает при помощи \"скрытого\" API. Внимательно изучите post-запросы.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        mail                  date                  site  \\\n",
      "0   xxx@x.ru  2016-10-21T00:00:00Z             adobe.com   \n",
      "0   xxx@x.ru  2016-10-29T00:00:00Z                vk.com   \n",
      "0   xxx@x.ru  2016-10-23T00:00:00Z             imesh.com   \n",
      "0   xxx@x.ru  2017-01-31T00:00:00Z      cdprojektred.com   \n",
      "0   xxx@x.ru  2017-02-14T00:00:00Z         cfire.mail.ru   \n",
      "0   xxx@x.ru  2017-02-14T00:00:00Z        parapa.mail.ru   \n",
      "0  yyy@y.com  2016-10-21T00:00:00Z          linkedin.com   \n",
      "0  yyy@y.com  2016-10-21T00:00:00Z             adobe.com   \n",
      "0  yyy@y.com  2016-10-23T00:00:00Z             imesh.com   \n",
      "0  yyy@y.com  2016-10-24T00:00:00Z           dropbox.com   \n",
      "0  yyy@y.com  2017-03-15T00:00:00Z        globalreach.eu   \n",
      "0  yyy@y.com  2017-03-01T00:00:00Z          rayli.com.cn   \n",
      "0  yyy@y.com  2017-03-24T00:00:00Z             youku.com   \n",
      "0  yyy@y.com  2017-11-04T00:00:00Z        myheritage.com   \n",
      "0  yyy@y.com  2018-02-18T00:00:00Z            netlog.com   \n",
      "0  yyy@y.com  2019-06-13T00:00:00Z             canva.com   \n",
      "0  yyy@y.com  2019-10-17T00:00:00Z             zynga.com   \n",
      "0  yyy@y.com  2020-01-03T00:00:00Z         azcentral.com   \n",
      "0  yyy@y.com  2020-05-28T00:00:00Z           wishbone.io   \n",
      "0  yyy@y.com  2021-02-11T00:00:00Z  forums.vkmonline.com   \n",
      "\n",
      "                                         description  \n",
      "0  In October of 2013, criminals penetrated Adobe...  \n",
      "0  Popular Russian social networking platform VKo...  \n",
      "0  In June 2016, a cache of over 51 million user ...  \n",
      "0  In March 2016, CDProjektRed.com.com's forum da...  \n",
      "0  In July and August of 2016, two criminals carr...  \n",
      "0  In July and August 2016, two criminals execute...  \n",
      "0  In 2012, online professional networking platfo...  \n",
      "0  In October of 2013, criminals penetrated Adobe...  \n",
      "0  In June 2016, a cache of over 51 million user ...  \n",
      "0  Cloud storage company Dropbox suffered a major...  \n",
      "0  In 2016, Global Reach Technology's database wa...  \n",
      "0  On an unconfirmed date, Chinese gossip site Ra...  \n",
      "0  Youku is a large Chinese video content company...  \n",
      "0  In October 2017, a customer database belonging...  \n",
      "0  Netlog (formerly known as Facebox and Bingbox)...  \n",
      "0  In May 2019, graphic-design site Canva's datab...  \n",
      "0  In September 2019, the game developer Zynga wa...  \n",
      "0  At an unconfirmed date, online Arizona newspap...  \n",
      "0  In January 2020, the online poll website Wishb...  \n",
      "0  At an unconfirmed date, the Russian-language m...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "URL = 'https://identityprotection.avast.com/v1/web/query/site-breaches/unauthorized-data'\n",
    "EMAIL = ['n.bor98@yandex.ru', 'xxx@x.ru', 'yyy@y.com']\n",
    "params = {\n",
    "    'emailAddresses': EMAIL\n",
    "}\n",
    "headers = {\n",
    "    'Vaar-Version': '0',\n",
    "    'Vaar-Header-App-Build-Version': '1.0.0',\n",
    "    'Vaar-Header-App-Product': 'hackcheck-web-avast',\n",
    "    'Vaar-Header-App-Product-Name': 'hackcheck-web-avast'\n",
    "}\n",
    "req = requests.post(URL, data=json.dumps(params), headers=headers)\n",
    "datas = pd.DataFrame()  \n",
    "diction  = json.loads(req.text)\n",
    "for mail in EMAIL:\n",
    "    if len(diction['summary'][mail]['breaches'])>0:\n",
    "        for breach in diction['summary'][mail]['breaches']:\n",
    "            row = {'mail': mail, 'date': diction['breaches'][str(breach)]['publishDate'],'site': diction['breaches'][str(breach)]['site'], 'description': diction['breaches'][str(breach)]['description']}\n",
    "            datas = pd.concat([datas, pd.DataFrame([row])]) \n",
    "print(datas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Дополнительная часть (необязательная)\n",
    "Написать скрипт, который будет получать 50 последних постов указанной группы во Вконтакте.\n",
    "Документация к API VK: https://vk.com/dev/methods , вам поможет метод wall.get\n",
    "\n",
    "GROUP = 'netology'  \n",
    "TOKEN = УДАЛЯЙТЕ В ВЕРСИИ ДЛЯ ПРОВЕРКИ, НА GITHUB НЕ ВЫКЛАДЫВАТЬ  \n",
    "В итоге должен формироваться датафрейм со столбцами: <дата поста> - <текст поста>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
